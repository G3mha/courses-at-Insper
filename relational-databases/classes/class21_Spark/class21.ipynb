{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando o ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O jeito mais simples de começar a trabalhar com Spark é instalar um container com tudo pronto! No site https://hub.docker.com/r/jupyter/pyspark-notebook vemos uma imagem Docker que já vem com `pyspark` e `jupyter lab`. Instale a imagem com o comando:\n",
    "\n",
    "```bash\n",
    "docker pull jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "Vamos iniciar o ambiente de trabalho com o comando `docker run`. Para isso precisamos tomar alguns cuidados:\n",
    "\n",
    "1) Temos que mapear nosso diretorio local de trabalho para um diretório interno do container, de modo que alterações feitas dentro do container (nesta pasta escolhida) sejam gravadas no nosso diretorio local. No container temos um usuário padrão com *username* `jovyan`. No *homedir* desse usuario temos uma pasta vazia `work`, que vai servir como local de mapeamento do nosso diretorio local de trabalho. Podemos então fazer esse mapeamendo com a opção `-v` do comando `docker run` da seguinte forma:\n",
    "\n",
    "```bash\n",
    "-v <diretorio>:/home/jovyan/work\n",
    "```\n",
    "\n",
    "onde `<diretorio>` representa seu diretorio local de trabalho.\n",
    "\n",
    "2) Para acessar o `jupyter notebook` e o *dashboard* do Spark a partir do nosso *browser* favorito temos que abrir algumas portas do container com a opção `-p`. As portas são `8888` (para o próprio `jupyter notebook`) e `4040` (para o *dashboard* do Spark). Ou seja, adicionaremos às opções do `docker run`o seguinte:\n",
    "\n",
    "```bash\n",
    "-p 8888:8888 -p 4040:4040\n",
    "```\n",
    "\n",
    "Desta forma, ao acessar `localhost:8888` na nossa máquina, estaremos acessando o servidor Jupyter na porta 8888 interna do container.\n",
    "\n",
    "3) Vamos iniciar o container no modo interativo, e vamos especificar que o container deve ser encerrado ao fechar o servidor Jupyter. Faremos isso com as opções `-it` e `-rm`\n",
    "\n",
    "Antes de executar, garanta que as portas 4040 e 8888 estão livres (sem jupyter já executando) ou altere o comando. Ainda, esteja na pasta da aula ao executar, assim apenas ela será exposta ao container.\n",
    "\n",
    "Portanto, o comando completo que eu uso na minha máquina Linux para iniciar o container é:\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "    -it \\\n",
    "    --rm \\\n",
    "    -p 8888:8888 \\\n",
    "    -p 4040:4040 \\\n",
    "    -v \"`pwd`\":/home/jovyan/work \\\n",
    "    jupyter/pyspark-notebook\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Se estiver no Windows estes comandos, utilize:\n",
    "\n",
    "- No Powershell: `docker run -it --rm -p 8888:8888 -p 4040:4040 -v ${PWD}:/home/jovyan/work jupyter/pyspark-notebook`\n",
    "\n",
    "- No Prompt de comando: `docker run -it --rm -p 8888:8888 -p 4040:4040 -v %cd%:/home/jovyan/work jupyter/pyspark-notebook`\n",
    "\n",
    "\n",
    "Para facilitar a vida eu coloco esse comando em um arquivo `inicia.sh`. Engenheiros, façam do jeito que preferirem!\n",
    "\n",
    "Agora abra esse notebook lá no container utilizando o link **com o token** que é exibido ao executar o comando!\n",
    "\n",
    "**Importante:** Se você já estiver visualizando esse notebook via Jupyter, pode ser necessário encerrar o processo para que o comando acima funcione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciando o Spark\n",
    "\n",
    "Vamos iniciar o ambiente Spark. Para isso vamos:\n",
    "\n",
    "1) Criar um objeto de configuração do ambiente Spark. Nossa configuração será simples: vamos especificar que o nome da nossa aplicação Spark é \"Minha aplicação\", e que o *master node* é a máquina local, usando todos os *cores* disponíveis. Aplicações reais de Spark são configuradas de modo ligeiramente diferente: ao especificar o *master node* passamos uma URL real, com o endereço do nó gerente do *cluster* Spark.\n",
    "\n",
    "2) Vamos criar um objeto do tipo `SparkContext` com essa configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MinhaAplicacao\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `SparkContext` é a nossa porta de entrada para o cluster Spark, ele será a raiz de todas as nossas operações com o Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.102.24.211:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MinhaAplicacao</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=MinhaAplicacao>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O link acima para a Spark UI provavelmente não funcionará porque ele se refere à porta 4040 interna do container (portanto a URL está com endereço interno). Porém fizemos o mapeamento da porta 4040 interna para a porta 4040 externa, logo você pode acessar o *dashboard* do Spark para monitorar seus *jobs* no endereço http://localhost:4040\n",
    "\n",
    "<center><img src=\"./spark_dashboard.png\" width=800/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto, assim você vai conseguir testar seus programas com facilidade!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalhando com RDDs\n",
    "\n",
    "Esse é o jeito \"Apache raiz\": Resilient Distributed Datasets (RDDs). Este é o principal objeto de processamento do Spark.\n",
    "\n",
    "Um RDD é criado à partir do objeto `SparkContext`. Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um RDD também pode ser criado a partir de um dataset (claro!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('memorias.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que o rdd pode ser particionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de partições:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantidade de partições: \", rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos imaginar um RDD como uma coleção de itens, similar a uma grande lista. O que vem em cada item depende do arquivo original de dados. Para arquivos de texto, cada linha é um item.\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html e sobre os RDDs em https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Açoes e Transformações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objeto `rdd` é repleto de métodos para definir pipelines computacionais. Estes métodos se dividem em *actions* e *transformations*.\n",
    "\n",
    "*Transformations* são métodos que atuam no RDD e devolvem um \"novo\" RDD que está conectado ao RDD antigo. Por exemplo, vamos usar a *transformation* `map` para inverter a sequência de letras em cada linha: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverte_linha(linha):\n",
    "    return linha[::-1]\n",
    "\n",
    "rdd2 = rdd.map(inverte_linha)\n",
    "rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você consultar a Web UI do Spark (http://localhost:4040) vai ver que nada ainda aconteceu. Isto é assim porque o Spark é *lazy*: a computação só acontece quando um resultado de uma sequência de *transformations* é demandado por uma *action*.\n",
    "\n",
    "As *actions* são métodos que retornam dados para o seu programa. Por exemplo, a *action* `count` retorna o número de itens no RDD, e a *action* `take` permite coletar alguns itens no inicio do RDD, bem útil para debugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8843"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confira a Web UI e veja que agora temos um novo job completo!\n",
    "\n",
    "Vamos espiar as primeiras 20 linhas do documento original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Project Gutenberg's Memorias Postumas de Braz Cubas, by Machado de Assis\",\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere in the United States and most',\n",
       " 'other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever.  You may copy it, give it away or re-use it under the terms of',\n",
       " 'the Project Gutenberg License included with this eBook or online at',\n",
       " \"www.gutenberg.org.  If you are not located in the United States, you'll have\",\n",
       " 'to check the laws of the country where you are located before using this ebook.',\n",
       " '',\n",
       " 'Title: Memorias Postumas de Braz Cubas',\n",
       " '',\n",
       " 'Author: Machado de Assis',\n",
       " '',\n",
       " 'Release Date: June 2, 2017 [EBook #54829]',\n",
       " '',\n",
       " 'Language: Portuguese',\n",
       " '',\n",
       " '',\n",
       " '*** START OF THIS PROJECT GUTENBERG EBOOK MEMORIAS POSTUMAS DE BRAZ CUBAS ***',\n",
       " '']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E agora as 20 primeiras linhas após a inversão de linha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"sissA ed odahcaM yb ,sabuC zarB ed samutsoP sairomeM s'grebnetuG tcejorP\",\n",
       " '',\n",
       " 'tsom dna setatS detinU eht ni erehwyna enoyna fo esu eht rof si kooBe sihT',\n",
       " 'snoitcirtser on tsomla htiw dna tsoc on ta dlrow eht fo strap rehto',\n",
       " 'fo smret eht rednu ti esu-er ro yawa ti evig ,ti ypoc yam uoY  .reveostahw',\n",
       " 'ta enilno ro kooBe siht htiw dedulcni esneciL grebnetuG tcejorP eht',\n",
       " \"evah ll'uoy ,setatS detinU eht ni detacol ton era uoy fI  .gro.grebnetug.www\",\n",
       " '.koobe siht gnisu erofeb detacol era uoy erehw yrtnuoc eht fo swal eht kcehc ot',\n",
       " '',\n",
       " 'sabuC zarB ed samutsoP sairomeM :eltiT',\n",
       " '',\n",
       " 'sissA ed odahcaM :rohtuA',\n",
       " '',\n",
       " ']92845# kooBE[ 7102 ,2 enuJ :etaD esaeleR',\n",
       " '',\n",
       " 'eseugutroP :egaugnaL',\n",
       " '',\n",
       " '',\n",
       " '*** SABUC ZARB ED SAMUTSOP SAIROMEM KOOBE GREBNETUG TCEJORP SIHT FO TRATS ***',\n",
       " '']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora configurar o insperautograder para que funcione dentro do container do Docker. Primeiro, instale e importe a biblioteca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/macielcalebe/insperautograding.git\n",
      "  Cloning https://github.com/macielcalebe/insperautograding.git to /private/var/folders/x3/bbk8nj6s67s9nv5gvt6q6gxh0000gn/T/pip-req-build-nkuvjgq0\n",
      "  Running command git clone -q https://github.com/macielcalebe/insperautograding.git /private/var/folders/x3/bbk8nj6s67s9nv5gvt6q6gxh0000gn/T/pip-req-build-nkuvjgq0\n",
      "  Resolved https://github.com/macielcalebe/insperautograding.git to commit acdda51152774d9e1a979b426e41daa7a8a7793c\n",
      "Requirement already satisfied: python-dotenv in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from insperautograder==0.2.0) (1.0.0)\n",
      "Requirement already satisfied: requests in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from insperautograder==0.2.0) (2.31.0)\n",
      "Requirement already satisfied: ipython in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from insperautograder==0.2.0) (8.18.1)\n",
      "Requirement already satisfied: ipywidgets in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from insperautograder==0.2.0) (8.1.1)\n",
      "Requirement already satisfied: decorator in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from ipython->insperautograder==0.2.0) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from ipython->insperautograder==0.2.0) (3.0.41)\n",
      "Requirement already satisfied: traitlets>=5 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from ipython->insperautograder==0.2.0) (5.14.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from ipython->insperautograder==0.2.0) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from ipython->insperautograder==0.2.0) (0.1.6)\n",
      "Requirement already satisfied: exceptiongroup in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from ipython->insperautograder==0.2.0) (1.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from ipython->insperautograder==0.2.0) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from ipython->insperautograder==0.2.0) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from ipython->insperautograder==0.2.0) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from jedi>=0.16->ipython->insperautograder==0.2.0) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from pexpect>4.3->ipython->insperautograder==0.2.0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->insperautograder==0.2.0) (0.2.12)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from ipywidgets->insperautograder==0.2.0) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from ipywidgets->insperautograder==0.2.0) (3.0.9)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from ipywidgets->insperautograder==0.2.0) (0.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from requests->insperautograder==0.2.0) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from requests->insperautograder==0.2.0) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from requests->insperautograder==0.2.0) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from requests->insperautograder==0.2.0) (3.3.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from stack-data->ipython->insperautograder==0.2.0) (2.0.1)\n",
      "Requirement already satisfied: pure-eval in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from stack-data->ipython->insperautograder==0.2.0) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from stack-data->ipython->insperautograder==0.2.0) (2.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/enriccogemha/Developer/courses-at-Insper/relational_databases/venv/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython->insperautograder==0.2.0) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/macielcalebe/insperautograding.git\n",
    "\n",
    "import insperautograder.jupyter as ia\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copie o arquivo .env para dentro da pasta que está vinculada ao container e veja se está funcionando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|    | Atividade            | De                        | Até                       |\n",
       "|---:|:---------------------|:--------------------------|:--------------------------|\n",
       "|  0 | newborn              | 2023-08-08 03:00:00+00:00 | 2023-08-16 02:59:59+00:00 |\n",
       "|  1 | select01             | 2023-08-08 03:00:00+00:00 | 2023-08-21 02:59:59+00:00 |\n",
       "|  2 | ddl                  | 2023-08-27 03:00:00+00:00 | 2023-09-02 02:59:59+00:00 |\n",
       "|  3 | dml                  | 2023-08-29 03:00:00+00:00 | 2023-09-04 02:59:59+00:00 |\n",
       "|  4 | agg_join             | 2023-09-03 03:00:00+00:00 | 2023-09-09 02:59:59+00:00 |\n",
       "|  5 | group_having         | 2023-09-03 03:00:00+00:00 | 2023-09-17 02:59:59+00:00 |\n",
       "|  6 | views                | 2023-09-11 03:00:00+00:00 | 2023-09-18 02:59:59+00:00 |\n",
       "|  7 | sql_review1          | 2023-09-13 03:00:00+00:00 | 2023-09-20 02:59:59+00:00 |\n",
       "|  8 | permissions          | 2023-09-20 03:00:00+00:00 | 2023-09-27 03:00:00+00:00 |\n",
       "|  9 | desafio_normalizacao | 2023-09-25 03:00:00+00:00 | 2023-10-02 03:00:00+00:00 |\n",
       "| 10 | ai_md_23_2           | 2023-10-09 03:00:00+00:00 | 2023-10-10 03:00:00+00:00 |\n",
       "| 11 | triggers             | 2023-10-19 03:00:00+00:00 | 2023-10-28 03:00:00+00:00 |\n",
       "| 12 | spark                | 2023-10-29 03:00:00+00:00 | 2023-11-08 15:00:00+00:00 |\n",
       "| 13 | functional           | 2023-10-29 03:00:00+00:00 | 2023-11-08 15:00:00+00:00 |\n",
       "| 14 | exercicios_spark     | 2023-11-02 03:00:00+00:00 | 2023-11-14 03:00:00+00:00 |\n",
       "| 15 | revisao_af_md_1      | 2023-11-27 03:00:00+00:00 | 2023-12-05 03:00:00+00:00 |\n",
       "| 16 | af_md_23_2           | 2023-11-30 19:00:00+00:00 | 2023-12-05 02:00:00+00:00 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "ia.tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|    | Atividade   | Exercício   |   Peso |   Nota |\n",
       "|---:|:------------|:------------|-------:|-------:|\n",
       "|  0 | spark       | desafio1    |      1 |     10 |\n",
       "|  1 | spark       | desafio2    |      1 |     10 |\n",
       "|  2 | spark       | ex01        |      1 |     10 |\n",
       "|  3 | spark       | ex02        |      1 |     10 |\n",
       "|  4 | spark       | ex03        |      1 |     10 |\n",
       "|  5 | spark       | ex04        |      1 |     10 |\n",
       "|  6 | spark       | ex05        |      1 |     10 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ia.grades(task=\"spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos discutir algumas ações e transformações úteis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `map`\n",
    "\n",
    "A transformação `map` recebe uma função e aplica esta função a cada item do RDD. A função deve receber um item e retornar um unico item. Como exemplo temos o uso de `map` acima para inverter a ordem das letras de cada linha.\n",
    "\n",
    "Note que no `map`, o RDD resultante tem o mesmo numero de elementos do RDD original.\n",
    "\n",
    "**Exercicio 1**: Crie uma função chamada conta_letras que recebe um rdd e retorna o rdd transformado onde cada linha tenha a contagem de letras da linha recebida. Por exemplo, a linha \"abacaxi\" deve ser transformada em 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 0, 74, 67, 74, 67, 76, 79, 0, 38, 0, 24, 0, 41, 0, 20, 0, 0, 77, 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resultado_esperado: [72, 0, 74, 67, 74, 67, 76, 79, 0, 38, 0, 24, 0, 41, 0, 20, 0, 0, 77, 0 ...\n",
    "\n",
    "def conta_letras(rdd):\n",
    "    # Count the number of characters in each line of RDD\n",
    "    rdd = rdd.map(lambda x: len(x))\n",
    "    # Return a new RDD with the number of characters in each line\n",
    "    return rdd\n",
    "\n",
    "conta_letras(rdd).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7494182800047c7ae63a67bf70ac591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Enviar ex01', style=ButtonStyle()), Output()), _dom_classes=('widget…"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia.sender(answer=\"conta_letras\", task=\"spark\", question=\"ex01\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 2**: Crie uma função chamada transforma_minusculas que recebe um rdd e retorna o rdd transformado onde cada linha tenha todas as letras em minúsculas. Por exemplo, a linha \"AbaCaxi HOJE\" deve ser transformada em \"abacaxi hoje\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"project gutenberg's memorias postumas de braz cubas, by machado de assis\",\n",
       " '',\n",
       " 'this ebook is for the use of anyone anywhere in the united states and most',\n",
       " 'other parts of the world at no cost and with almost no restrictions',\n",
       " 'whatsoever.  you may copy it, give it away or re-use it under the terms of',\n",
       " 'the project gutenberg license included with this ebook or online at',\n",
       " \"www.gutenberg.org.  if you are not located in the united states, you'll have\",\n",
       " 'to check the laws of the country where you are located before using this ebook.',\n",
       " '',\n",
       " 'title: memorias postumas de braz cubas',\n",
       " '',\n",
       " 'author: machado de assis',\n",
       " '',\n",
       " 'release date: june 2, 2017 [ebook #54829]',\n",
       " '',\n",
       " 'language: portuguese',\n",
       " '',\n",
       " '',\n",
       " '*** start of this project gutenberg ebook memorias postumas de braz cubas ***',\n",
       " '']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transforma_minusculas(rdd):\n",
    "    # Transform all characters to lowercase\n",
    "    rdd = rdd.map(lambda x: x.lower())\n",
    "    # Return a new RDD with all characters in lowercase\n",
    "    return rdd\n",
    "    \n",
    "transforma_minusculas(rdd).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe7b8da20e744579820cd7650a10c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Enviar ex02', style=ButtonStyle()), Output()), _dom_classes=('widget…"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia.sender(answer=\"transforma_minusculas\", task=\"spark\", question=\"ex02\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `flatMap`\n",
    "\n",
    "Esta transformação é similar ao `map`, porém a função passada para o `flatMap` pode retornar zero ou mais itens. Estes itens então são concatenados e o RDD resultante acaba tendo um número de itens diferente do RDD original.\n",
    "\n",
    "Por exemplo: suponha que queremos gerar uma lista de palavras do documento. Podemos fazê-lo da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " \"gutenberg's\",\n",
       " 'memorias',\n",
       " 'postumas',\n",
       " 'de',\n",
       " 'braz',\n",
       " 'cubas,',\n",
       " 'by',\n",
       " 'machado',\n",
       " 'de',\n",
       " 'assis',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " 'and',\n",
       " 'most',\n",
       " 'other',\n",
       " 'parts',\n",
       " 'of',\n",
       " 'the']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def separa_palavras(linha):\n",
    "    # Split `linha` into words\n",
    "    return linha.strip().split()\n",
    "\n",
    "rdd_flatMap = rdd.map(lambda x: x.lower()).flatMap(separa_palavras)\n",
    "rdd_flatMap.take(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja a contagem de palavras no `rdd_flatMap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64713"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_flatMap.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare com o numero de itens no RDD original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8843"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 3**: Crie uma função chamada gera_bigramas que recebe um rdd e retorna o rdd transformado onde cada linha contém uma **tupla** com um *bigrama*: sequências de duas palavras consecutivas.\n",
    "\n",
    "Gere cada bigrama como uma **tupla** (p[i], p[i+1]). \n",
    "\n",
    "Sua função deve utilizar o comando `flatMap`. Para esse exercício é permitido utilizar um comando for dentro do flatmap. Por que isso não prejudica a performance?\n",
    "\n",
    "Além disso, teremos um pequeno problema nos bigramas - que problema é esse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', \"gutenberg's\"),\n",
       " (\"gutenberg's\", 'memorias'),\n",
       " ('memorias', 'postumas'),\n",
       " ('postumas', 'de'),\n",
       " ('de', 'braz'),\n",
       " ('braz', 'cubas,'),\n",
       " ('cubas,', 'by'),\n",
       " ('by', 'machado'),\n",
       " ('machado', 'de'),\n",
       " ('de', 'assis'),\n",
       " ('this', 'ebook'),\n",
       " ('ebook', 'is'),\n",
       " ('is', 'for'),\n",
       " ('for', 'the'),\n",
       " ('the', 'use'),\n",
       " ('use', 'of'),\n",
       " ('of', 'anyone'),\n",
       " ('anyone', 'anywhere'),\n",
       " ('anywhere', 'in'),\n",
       " ('in', 'the')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gera_bigramas(rdd):\n",
    "    # Transform all characters to lowercase\n",
    "    rdd = rdd.map(lambda x: x.lower())\n",
    "    \n",
    "    # Create subfunction to mount bigrams per line\n",
    "    def create_bigrams_per_line(line):\n",
    "        # Split line into words\n",
    "        words = line.split()\n",
    "        # Create bigrams\n",
    "        bigrams = [(words[i], words[i+1]) for i in range(len(words) - 1)]\n",
    "        return bigrams\n",
    "\n",
    "    return rdd.flatMap(create_bigrams_per_line)\n",
    "\n",
    "rdd_bigramas = gera_bigramas(rdd)\n",
    "rdd_bigramas.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d339a92e9644bcba4115b626f0ccf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Enviar ex03', style=ButtonStyle()), Output()), _dom_classes=('widget…"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia.sender(answer=\"gera_bigramas\", task=\"spark\", question=\"ex03\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O problema é que bigramas que cruzam linhas não serão contabilizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `filter`\n",
    "\n",
    "A transformação `filter` recebe uma função que recebe um item e retorna True se o item deve ser mantido, e False se deve ser ignorado. Por exemplo, suponha que temos uma lista de palavras proibidas e queremos manter apenas as palavras permitidas da nossa lista de palavras em `rdd_flatMap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " \"gutenberg's\",\n",
       " 'memorias',\n",
       " 'postumas',\n",
       " 'de',\n",
       " 'braz',\n",
       " 'cubas,',\n",
       " 'by',\n",
       " 'machado',\n",
       " 'de',\n",
       " 'assis',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " 'and',\n",
       " 'most',\n",
       " 'other',\n",
       " 'parts',\n",
       " 'of',\n",
       " 'the']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_flatMap.take(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project',\n",
       " \"gutenberg's\",\n",
       " 'memorias',\n",
       " 'postumas',\n",
       " 'de',\n",
       " 'cubas,',\n",
       " 'by',\n",
       " 'machado',\n",
       " 'de',\n",
       " 'this',\n",
       " 'ebook',\n",
       " 'is',\n",
       " 'for',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'anyone',\n",
       " 'anywhere',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " 'and',\n",
       " 'most',\n",
       " 'other',\n",
       " 'parts',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'at']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eh_palavra_permitida(palavra):\n",
    "    palavras_proibidas = set(['braz', 'assis'])\n",
    "    return palavra not in palavras_proibidas\n",
    "\n",
    "rdd_limpo = rdd_flatMap.filter(eh_palavra_permitida)\n",
    "rdd_limpo.take(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `reduce`\n",
    "\n",
    "A *action* `reduce` apresenta o mesmo comportamento da operação `reduce` da biblioteca `functools` do Python. Ela recebe uma função de dois argumentos que retorna apenas um valor. O comportamento de `reduce` é aplicar esta função aos vários elementos do RDD de modo sucessivo, visando \"reduzir\" o RDD inteiro a um único valor, que é então retornado para o programa principal.\n",
    "\n",
    "Por exemplo, suponha que desejamos calcular a soma dos comprimentos de palavras. Podemos fazer o seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317610"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_flatMap.map(lambda x: len(x)).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que como `reduce` é uma *action*, temos um resultado concreto.\n",
    "\n",
    "**Exercício 4**: Crie uma função chamada maior_palindromo que recebe um rdd com o texto e retorna a maior palavra palíndroma do texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..........'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maior_palindromo(rdd):\n",
    "    # Transform all characters to lowercase\n",
    "    rdd = rdd.map(lambda x: x.lower())\n",
    "    # Filter blank lines\n",
    "    rdd = rdd.filter(lambda x: len(x) > 0)\n",
    "    # Create subfunction to check if a word is a palindrome\n",
    "    def is_palindrome(word):\n",
    "        return word == word[::-1]\n",
    "    # Filter palindromes\n",
    "    rdd = rdd.filter(is_palindrome)\n",
    "    # Filter the blank spaces between words\n",
    "    rdd = rdd.filter(lambda x: len(x.split()) == 1)\n",
    "    # Return the longest palindrome\n",
    "    return rdd.reduce(lambda x, y: x if len(x) > len(y) else y)\n",
    "\n",
    "maior_palindromo(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c16b4193a843278b20a9ac696c1e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Enviar ex04', style=ButtonStyle()), Output()), _dom_classes=('widget…"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia.sender(answer=\"maior_palindromo\", task=\"spark\", question=\"ex04\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pares chave-valor\n",
    "\n",
    "Sempre que o RDD consistir em itens que sejam tuplas de dois elementos, Spark vai considerar que temos um par chave-valor por item. Isso nos permite realizar agregações por chave, abrindo a possibilidade de coletar dados agregados mais interessantes. \n",
    "\n",
    "#### `reduceByKey`\n",
    "Para essa tarefa, a transformação mais importante é a `reduceByKey`. Esta operação realiza uma agregação por chave (gerando uma lista de valores para aquela chave), e realiza uma redução da lista dos valores associados à chave.\n",
    "\n",
    "Por exemplo, o *hello world* do Spark: o conta-palavras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2593),\n",
       " ('de', 2139),\n",
       " ('que', 2097),\n",
       " ('e', 1944),\n",
       " ('o', 1831),\n",
       " ('não', 1063),\n",
       " ('um', 987),\n",
       " ('do', 744),\n",
       " ('da', 656),\n",
       " ('uma', 652)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_flatMap \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .takeOrdered(10, lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 5**: Crie uma função chamada conta_bigramas que recebe um rdd com as tuplas de bigramas e retorna pares chave e valor dos 10 bigramas mais populares.\n",
    "\n",
    "**Atenção:** Apesar da resposta aparecer como se fossem listas, os elementos apresentados são tuplas em python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('.', '.'), 159),\n",
       " (('de', 'um'), 148),\n",
       " (('que', 'a'), 133),\n",
       " (('que', 'o'), 125),\n",
       " (('que', 'não'), 117),\n",
       " (('que', 'me'), 116),\n",
       " (('o', 'que'), 111),\n",
       " (('e', 'a'), 107),\n",
       " (('que', 'eu'), 107),\n",
       " (('de', 'uma'), 100)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conta_bigramas(rdd_bigramas):\n",
    "    # Count the number of occurrences of each bigram\n",
    "    rdd = rdd_bigramas.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "    # Return a new RDD with the number of occurrences of each bigram\n",
    "    return rdd.takeOrdered(10, lambda x: -x[1])\n",
    "\n",
    "conta_bigramas(gera_bigramas(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b0c30a1f8742929aa407884e7f1e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Enviar ex05', style=ButtonStyle()), Output()), _dom_classes=('widget…"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia.sender(answer=\"conta_bigramas\", task=\"spark\", question=\"ex05\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `join`\n",
    "\n",
    "A transformação `join` realiza o *inner join* de dois RDDs. Por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('b', (20, 'chuchu')),\n",
       " ('b', (20, 'tomate')),\n",
       " ('b', (30, 'chuchu')),\n",
       " ('b', (30, 'tomate')),\n",
       " ('a', (10, 'banana')),\n",
       " ('a', (10, 'abacate'))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_a = sc.parallelize([('a', 10), ('b', 20), ('b', 30)])\n",
    "rdd_b = sc.parallelize([('a', 'banana'), ('a', 'abacate'), ('b', 'chuchu'), ('b', 'tomate')])\n",
    "rdd_c = rdd_a.join(rdd_b)\n",
    "\n",
    "rdd_c.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desafio 1**: Crie uma função chamada desafio1 que recebe um rdd e o transforma de modo que para cada bigrama, temos a seguinte informação:\n",
    "\n",
    "(bigrama, contagem_bigrama, contagem_palavra_1, contagem_palavra_2)\n",
    "\n",
    "**Atenção:** Apesar da resposta aparecer como se fossem listas, os elementos apresentados são tuplas em python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('.', '.'), 159, 187, 187),\n",
       " (('de', 'um'), 148, 2139, 987),\n",
       " (('que', 'a'), 133, 2097, 2593),\n",
       " (('que', 'o'), 125, 2097, 1831),\n",
       " (('que', 'não'), 117, 2097, 1063)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def desafio1(rdd):\n",
    "    # Transform all characters to lowercase\n",
    "    rdd = rdd.map(lambda x: x.lower())\n",
    "    # Filter blank lines\n",
    "    rdd = rdd.filter(lambda x: len(x) > 0)\n",
    "\n",
    "    def separate_word(linha):\n",
    "        # Split `linha` into words\n",
    "        return linha.strip().split()\n",
    "    \n",
    "    # Separate words\n",
    "    rdd_words = rdd.flatMap(separate_word)\n",
    "    # Count the number of occurrences of each word\n",
    "    rdd_words = rdd_words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "    # Create a dictionary with the number of occurrences of each word\n",
    "    words = rdd_words.collectAsMap()\n",
    "\n",
    "    # Create subfunction to generate bigrams in a line\n",
    "    def generate_bigrams_line(line):\n",
    "        # Split line into words\n",
    "        words = line.split()\n",
    "        # Create bigrams\n",
    "        bigrams = [(words[i], words[i+1]) for i in range(len(words) - 1)]\n",
    "        return bigrams\n",
    "    \n",
    "    rdd = rdd.flatMap(generate_bigrams_line)\n",
    "\n",
    "    # Count the number of occurrences of each bigram\n",
    "    rdd = rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "    # Get the number of occurrences of each bigram in `rdd_words`\n",
    "    rdd = rdd.map(lambda x: (x[0], x[1], words[x[0][0]], words[x[0][1]]))\n",
    "    return rdd \n",
    "\n",
    "desafio1(rdd).takeOrdered(5, lambda x: -x[1])\n",
    "# (desafio1(rdd).take(10))\n",
    "# (desafio1(rdd)).filter(lambda x: x[0] == ('a', 'part')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f9a33b2d794472b7480d2311cbe22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Enviar desafio1', style=ButtonStyle()), Output()), _dom_classes=('wi…"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia.sender(answer=\"desafio1\", task=\"spark\", question=\"desafio1\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desafio 2**: Faça uma função chamada desafio2 que recebe um RDD no formato retornado pela função desafio1 e constrói um RDD com a seguinte informação:\n",
    "\n",
    "(bigrama, (contagem_bigrama) /((contagem_palavra_1 + K) * (contagem_palavra_2 + K))\n",
    "\n",
    "para K = 10\n",
    "\n",
    "Por fim, retorne os 15 bigramas com maior valor associado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('set', 'forth'), 0.024691358024691357),\n",
       " (('arma', 'virumque'), 0.020833333333333332),\n",
       " (('qui', \"l'eut\"), 0.02040816326530612),\n",
       " (('memorias', 'postumas'), 0.0196078431372549),\n",
       " (('web', 'site'), 0.01904761904761905),\n",
       " (('literary', 'archive'), 0.01890359168241966),\n",
       " (('virumque', 'cano'), 0.017857142857142856),\n",
       " (('memórias', 'pósthumas'), 0.01775147928994083),\n",
       " (('information', 'about'), 0.016339869281045753),\n",
       " (('archive', 'foundation'), 0.016304347826086956),\n",
       " (('phrase', '\"project'), 0.014814814814814815),\n",
       " (('\"project', 'gutenberg\"'), 0.014285714285714285),\n",
       " (('gutenberg', 'literary'), 0.014066496163682864),\n",
       " (('laura', 'natal'), 0.013888888888888888),\n",
       " (('rodriguez', '&'), 0.013888888888888888)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def desafio2(rdd_d1):\n",
    "    K = 10\n",
    "    rdd_d1 = rdd_d1.map(lambda x: (x[0], (x[1] /((x[2] + K) * (x[3] + K)))))\n",
    "    return rdd_d1.takeOrdered(15, lambda x: -x[1])\n",
    "\n",
    "desafio2(desafio1(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8b234cadf049909825d5d3a00c4451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Enviar desafio2', style=ButtonStyle()), Output()), _dom_classes=('wi…"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ia.sender(answer=\"desafio2\", task=\"spark\", question=\"desafio2\", answer_type=\"pycode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|    | Atividade   | Exercício   |   Peso |   Nota |\n",
       "|---:|:------------|:------------|-------:|-------:|\n",
       "|  0 | spark       | desafio1    |      1 |     10 |\n",
       "|  1 | spark       | desafio2    |      1 |     10 |\n",
       "|  2 | spark       | ex01        |      1 |     10 |\n",
       "|  3 | spark       | ex02        |      1 |     10 |\n",
       "|  4 | spark       | ex03        |      1 |     10 |\n",
       "|  5 | spark       | ex04        |      1 |     10 |\n",
       "|  6 | spark       | ex05        |      1 |     10 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify the grades obtained in the tasks\n",
    "ia.grades(task=\"spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gabarito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<div id=\"gab_ex3\">**Exercício 3**</div>**\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "```python\n",
    "def gera_bigramas(rdd):\n",
    "    return rdd.map(lambda x: x.lower()) \\\n",
    "        .map(lambda x: x.split()) \\\n",
    "        .flatMap(lambda x: [(x[i], x[i+1]) for i in range(len(x)-1)])\n",
    "```\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**<div id=\"gab_ex4\">**Exercício 5**</div>**\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "```python\n",
    "def conta_bigramas(rdd_bigramas):\n",
    "    rdd_conta_bigramas = rdd_bigramas.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "    return rdd_conta_bigramas.takeOrdered(10, lambda x: -x[1])\n",
    "```\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
