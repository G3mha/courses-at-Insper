{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando o ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O jeito mais simples de começar a trabalhar com Spark é instalar um container com tudo pronto! No site https://hub.docker.com/r/jupyter/pyspark-notebook vemos uma imagem Docker que já vem com `pyspark` e `jupyter lab`. Instale a imagem com o comando:\n",
    "\n",
    "```bash\n",
    "docker pull jupyter/pyspark-notebook\n",
    "```\n",
    "\n",
    "Vamos iniciar o ambiente de trabalho com o comando `docker run`. Para isso precisamos tomar alguns cuidados:\n",
    "\n",
    "1) Temos que mapear nosso diretorio local de trabalho para um diretório interno do container, de modo que alterações feitas dentro do container (nesta pasta escolhida) sejam gravadas no nosso diretorio local. No container temos um usuário padrão com *username* `jovyan`. No *homedir* desse usuario temos uma pasta vazia `work`, que vai servir como local de mapeamento do nosso diretorio local de trabalho. Podemos então fazer esse mapeamendo com a opção `-v` do comando `docker run` da seguinte forma:\n",
    "\n",
    "```bash\n",
    "-v <diretorio>:/home/jovyan/work\n",
    "```\n",
    "\n",
    "onde `<diretorio>` representa seu diretorio local de trabalho.\n",
    "\n",
    "2) Para acessar o `jupyter notebook` e o *dashboard* do Spark a partir do nosso *browser* favorito temos que abrir algumas portas do container com a opção `-p`. As portas são `8888` (para o próprio `jupyter notebook`) e `4040` (para o *dashboard* do Spark). Ou seja, adicionaremos às opções do `docker run`o seguinte:\n",
    "\n",
    "```bash\n",
    "-p 8888:8888 -p 4040:4040\n",
    "```\n",
    "\n",
    "Desta forma, ao acessar `localhost:8888` na nossa máquina, estaremos acessando o servidor Jupyter na porta 8888 interna do container.\n",
    "\n",
    "3) Vamos iniciar o container no modo interativo, e vamos especificar que o container deve ser encerrado ao fechar o servidor Jupyter. Faremos isso com as opções `-it` e `-rm`\n",
    "\n",
    "Antes de executar, garanta que as portas 4040 e 8888 estão livres (sem jupyter já executando) ou altere o comando. Ainda, esteja na pasta da aula ao executar, assim apenas ela será exposta ao container.\n",
    "\n",
    "Portanto, o comando completo que eu uso na minha máquina Linux para iniciar o container é:\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "    -it \\\n",
    "    --rm \\\n",
    "    -p 8888:8888 \\\n",
    "    -p 4040:4040 \\\n",
    "    -v \"`pwd`\":/home/jovyan/work \\\n",
    "    jupyter/pyspark-notebook\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Se estiver no Windows estes comandos, utilize:\n",
    "\n",
    "- No Powershell: `docker run -it --rm -p 8888:8888 -p 4040:4040 -v ${PWD}:/home/jovyan/work jupyter/pyspark-notebook`\n",
    "\n",
    "- No Prompt de comando: `docker run -it --rm -p 8888:8888 -p 4040:4040 -v %cd%:/home/jovyan/work jupyter/pyspark-notebook`\n",
    "\n",
    "\n",
    "Para facilitar a vida eu coloco esse comando em um arquivo `inicia.sh`. Engenheiros, façam do jeito que preferirem!\n",
    "\n",
    "Agora abra esse notebook lá no container!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciando o Spark\n",
    "\n",
    "Vamos iniciar o ambiente Spark. Para isso vamos:\n",
    "\n",
    "1) Criar um objeto de configuração do ambiente Spark. Nossa configuração será simples: vamos especificar que o nome da nossa aplicação Spark é \"Minha aplicação\", e que o *master node* é a máquina local, usando todos os *cores* disponíveis. Aplicações reais de Spark são configuradas de modo ligeiramente diferente: ao especificar o *master node* passamos uma URL real, com o endereço do nó gerente do *cluster* Spark.\n",
    "\n",
    "2) Vamos criar um objeto do tipo `SparkContext` com essa configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MinhaAplicacao\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir deste momento você pode monitorar seus *jobs* Spark em http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark: SQL\n",
    "\n",
    "Nas últimas aulas de Spark, aprendemos sobre RDD (Resilient Distributed Datasets), que é a estrutura de dados fundamental do Spark. Com o RDD, podemos executar operações paralelas e distribuídas em grandes conjuntos de dados em um cluster.\n",
    "\n",
    "<img src=\"cluster-overview.png\">\n",
    "\n",
    "Fonte: https://spark.apache.org/docs/latest/img/cluster-overview.png\n",
    "\n",
    "Aprendemos que os RDDs são imutáveis e podem ser criados a partir de dados armazenados em arquivos ou gerados por transformações de outros RDDs. Além disso, vimos como aplicar diferentes tipos de transformações, como `map`, `filter` e `reduce` para manipular nossos dados.\n",
    "\n",
    "Agora, vamos aprender sobre a interface de DataFrames do PySpark. Os DataFrames são uma abstração de alto nível construída em cima dos RDDs, que permitem a manipulação de dados estruturados de forma mais eficiente. Eles fornecem uma API mais fácil e intuitiva para trabalhar com dados tabulares, além de permitir a execução de consultas SQL diretamente nos dados.\n",
    "\n",
    "Sim, você não leu errado, conseguiremos utilizar SQL! Em nossas aulas, discutimos como geralmente servidores de banco de dados são otimizados para IO (leitura e escrita de dados) e não para processamento. Com o uso do Spark e a interface SQL, conseguiremos aplicar nossos conhecimentos em um ambiente otimizado para processamento de dados em larga escala enquanto utilizamos uma interface mais amigável, permitindo que as tarefas de análise sejam realizadas de forma mais rápida e eficiente.\n",
    "\n",
    "Com os DataFrames, podemos executar tarefas como filtragem, agregação, junção e ordenação dos dados com muito mais facilidade. Além disso, eles oferecem suporte a diversos formatos de arquivos, incluindo CSV, Parquet, JSON e avro.\n",
    "\n",
    "**Obs**: apesar de serem DataFrames, não são Pandas DataFrames! Apesar de podermos transformar estes DataFrames em Pandas, esta é uma operação que deve ser evitada ao máximo, pois assim perdemos a característica distribuída do Spark!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de Dados\n",
    "\n",
    "Iremos utilizar a base de dados **SF Bay Area Bike Share** do [Kaggle](https://www.kaggle.com/datasets/benhamner/sf-bay-area-bike-share?resource=download). Para fazer o download, acesse https://www.kaggle.com/datasets/benhamner/sf-bay-area-bike-share?resource=download\n",
    "\n",
    "Provavelmente o download pelo Kaggle irá demorar. Enquanto ele não finalize, utilize como alternativa o zip disponível no Blackboard. Ele possui os mesmos CSVs, exceto um gigantesco (`status.csv`)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeiro exemplo\n",
    "\n",
    "Vamos abrir o arquivo `station.csv`. Deixe ele em uma pasta `data` dentro da pasta da aula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_station = spark.read.csv(\"data/station.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos exibir algumas linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------------+-------------------+----------+--------+-----------------+\n",
      "| id|                name|               lat|               long|dock_count|    city|installation_date|\n",
      "+---+--------------------+------------------+-------------------+----------+--------+-----------------+\n",
      "|  2|San Jose Diridon ...|         37.329732|-121.90178200000001|        27|San Jose|         8/6/2013|\n",
      "|  3|San Jose Civic Ce...|         37.330698|        -121.888979|        15|San Jose|         8/5/2013|\n",
      "|  4|Santa Clara at Al...|         37.333988|        -121.894902|        11|San Jose|         8/6/2013|\n",
      "|  5|    Adobe on Almaden|         37.331415|          -121.8932|        19|San Jose|         8/5/2013|\n",
      "|  6|    San Pedro Square|37.336721000000004|        -121.894074|        15|San Jose|         8/7/2013|\n",
      "+---+--------------------+------------------+-------------------+----------+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_station.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma maneira alternativa de exibir de forma mais bonita! **Cuidado**: transformações para Pandas em geral devem ser evitadas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_station.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Para ver o *schema* do DataFrame, utilize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_station.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar `count` e `columns` para descobrir o *shape* do DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_station.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_station.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('{} linhas e {} colunas'.format(df_station.count(), len(df_station.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos encontrar todas as cidades diferentes nas quais temos estações em nossa base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "distinct_cities = df_station.select(col(\"city\")).distinct()\n",
    "\n",
    "distinct_cities.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acesse a documentação do `select` em \n",
    "https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.select.html\n",
    "e do `col` em https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.col.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar Databases\n",
    "\n",
    "Vamos tentar trabalhar de forma mais semelhante ao praticado no MySQL!\n",
    "\n",
    "Será que conseguimos criar nossas próprias databases e tabelas?!\n",
    "\n",
    "Sim! Para criar uma database `bike`, podemos fazer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bike\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar Tabelas\n",
    "\n",
    "Em um ambiente de processamento em larga escala como o Spark, as tabelas podem surgir tanto em um processo de ingestão, quando arquivos externos a base são ingeridos para disponibilizar dados aos cientistas ou analistas de dados, quanto quando tabelas são reprocessadas para gerar visões necessárias para satisfazer algum projeto (Ex: um projeto Machine Learning para predição de custos).\n",
    "\n",
    "Para exportar um DataFrame como tabela, podemos fazer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_station.write.mode('overwrite').saveAsTable(\"bike.station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Para criar uma view, utilize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_station.createOrReplaceTempView(\"view_station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vá até seu navegador de arquivos (Windows Explorer, Nautilius). Verifique que foi criada a pasta `spark-warehouse`. Acesse esta pasta e confira seu conteúdo!\n",
    "\n",
    "Perceba que nossas tabelas foram salvas em arquivos `.parquet`. O Apache Parquet é um formato de arquivo open-source projetado para armazenar dados em colunas. Ele é otimizado para consulta e processamento eficiente de grandes quantidades de dados estruturados e semi-estruturados, especialmente em ambientes de big data. Ao contrário de outros formatos de arquivo que armazenam dados em linhas, o Parquet armazena dados em colunas, o que permite uma compressão mais eficiente e um desempenho de consulta mais rápido.\n",
    "\n",
    "<img src=\"parquet.gif\">\n",
    "Fonte: https://parquet.apache.org/images/FileLayout.gif\n",
    "\n",
    "O Parquet foi projetado para trabalhar bem com frameworks distribuídos como Hadoop, Spark e Hive, permitindo que os usuários consultem e processem dados em escala. Ele também suporta tipos de dados complexos, como arrays e estruturas aninhadas, o que o torna flexível o suficiente para lidar com uma ampla variedade de casos de uso. Em resumo, o Apache Parquet é uma ferramenta útil para gerenciar e processar grandes volumes de dados de forma eficiente e escalável.\n",
    "\n",
    "Veja mais em https://parquet.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos especificar partições na criação das tabelas. Considere um caso em que a tabela de `station` será dividida pelas cidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_station.write.partitionBy(\"city\").mode(\"overwrite\").saveAsTable(\"bike.station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volte ao navegador de arquivos e perceba as alterações. Deve ter sido criada uma pasta por cidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Query\n",
    "\n",
    "Pronto, agora podemos utilizar queries como fizemos no MySQL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minha_query = \"\"\"\n",
    "SELECT *\n",
    "  FROM bike.station\n",
    " LIMIT 2\n",
    " \"\"\"\n",
    "\n",
    "df_exemplo = spark.sql(minha_query)\n",
    "df_exemplo.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando a view..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minha_query = \"\"\"\n",
    "SELECT s.name,\n",
    "       s.city,\n",
    "       s.dock_count\n",
    "  FROM view_station s\n",
    " LIMIT 3\n",
    " \"\"\"\n",
    "\n",
    "df_exemplo = spark.sql(minha_query)\n",
    "df_exemplo.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 1**: Crie na base `bike` uma tabela `weather` a partir do arquivo `weather.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seu código AQUI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 2**: Crie na base `bike` uma tabela `trip` a partir do arquivo `trip.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seu código AQUI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 3**: Crie na base `bike` uma tabela `status` a partir do arquivo `status.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seu código AQUI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 4**: Conte a quantidade de linhas em cada tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu código AQUI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 5**: Conte a quantidade de corridas (`trip`) com cada estação como **origem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu código AQUI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 6**: Conte a quantidade de corridas (`trip`) com cada estação como **destino**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu código AQUI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins\n",
    "\n",
    "Podemos utilizar **join** e os demais recursos (funções de agregação, agrupamentos...) que utilizávamos no MySQL. Veja um exemplo onde retornaremos algumas informações da estação fazendo um **INNER JOIN** e retornando as informações de `status` da estação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minha_query = \"\"\"\n",
    "SELECT s.name,\n",
    "       s.city,\n",
    "       s.dock_count,\n",
    "       t.*\n",
    "  FROM bike.station s,\n",
    "       bike.status t\n",
    "WHERE t.station_id = s.id\n",
    " LIMIT 3\n",
    " \"\"\"\n",
    "\n",
    "df_info_st = spark.sql(minha_query)\n",
    "df_info_st.show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você irá perceber uma certa demora para retorno dos resultados. Estamos em um ambiente que propicia processamento em larga escala de forma distribuída. Conseguimos recuperar, agrupar, resumir e até treinar modelos de Machine Learning, mas isto virá com um custo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício 7**: Crie um DataFrame a partir de uma **Query SQL** que retorne o `id`, `name`, `city` além da quantidade média de bicicletas disponíveis nos `status` de cada estação.\n",
    "\n",
    "Dicas: junção e agrupamento!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu código AQUI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolvendo com API DataFrame\n",
    "\n",
    "Vamos ver como resolver o Exercício 4 utilizando a API de DataFrames.\n",
    "\n",
    "Inicialmente, iremos ler a tabela de status. Já temos um DataFrame para esta tabela, entretanto, iremos criar outro para fins ditáticos! Perceba que desta vez iremos fazer o processo inverso e ler a partir da tabela! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_status1 = spark.read.table(\"bike.status\")\n",
    "df_status1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, lemos a tabela de estações em um DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_station1 = spark.read.table(\"bike.station\")\n",
    "df_station1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fazer o join, utilizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_join = df_station1.join(df_status1, col(\"id\") == col(\"station_id\"), \"inner\")\n",
    "df_join.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E utilizamos `avg` como função de agregação para obter os resultados desejados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df_join.groupBy(\"id\", \"name\", \"city\").agg(avg(\"bikes_available\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceba que os resultados serão os mesmos da opção resolvida com SQL.\n",
    "\n",
    "Em geral, a escolha pela interface de SQL ou DataFrame em um ambiente de trabalho com Spark levará em consideração questões como padronização e a familiaridade dos desenvolvedores com uma interface ou outra. Seja qual a escolha, o tempo de processamento também deverá ser muito parecido, uma vez que o Spark realiza uma \"tradução\" no momento de execução."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
